{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 911-Crime detection using ResNet50 Model\n",
    "\n",
    "# GCP connection\n",
    "\n",
    "###### By Project ATMA Team"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For this model, we decided to use the ResNet50 (Deep Residual Networks with 50 layers) to help us determine whether a given short video input can be classified as an active crime.\n",
    "\n",
    "In this python notebook, we will explore the use of features in order to re-train a model's trained weights (in the dense layers and some layers from the original model) to recognize crimes without the need to train the full model from scratch (which usually requires a large amount of data).\n",
    "\n",
    "ResNet has an exotic architecture also called \"network on network architecture\". Such micro-architecture modules refer to the building blocks that make up the network. Together with the standard layers, a macho-architecture is formed and \"residual learning\" is introduced. Ever since introduced by He et al., ResNets have demostrated that deep networks can be trained with a standard SGD (Stochastic Gradient Descent) optimizer.\n",
    "\n",
    "\"Deep convolutional neural networks have led to a series of breakthroughs for image classification. Many other visual recognition tasks have also greatly benefited from very deep models. Over the years there is a trend to go more deeper to solve more complex tasks and to also increase/improve the classification/recognition accuracy. But, as we go deeper, the training of neural network becomes difficult; the accuracy saturates and even degrades. Residual Learning tries to solve both these problems.\n",
    "In general, in a deep convolutional neural network, several layers are stacked and are trained to the task at hand. The network learns several low/mid/high level features at the end of its layers. In residual learning, instead of trying to learn some features, we try to learn some residual. Residual can be simply understood as subtraction of feature learned from input of that layer. ResNet does this using shortcut connections (directly connecting input of nth layer to some (n+x)th layer. It has proved that training this form of networks is easier than training simple deep convolutional neural networks and also the problem of degrading accuracy is resolved.\" Writen By Kartik Ordugo, https://www.quora.com/What-is-the-deep-neural-network-known-as-%E2%80%9CResNet-50%E2%80%9D\n",
    "\n",
    "ResNets take activations from one layer and feed it into another layer much deeper in the network. This is called \"Skip connections\". they work because the identity function is easy for residual blocks to learn, as the same input is used and transferred into a deeper layer and in the case that the weights/bias fails to change the input (by applying weight/bias decay), the relu goes back to the skipped input. Thereby learning the identity function.\n",
    "\n",
    "* Deep Residual Learning for Image Recognition by He et al.\n",
    "    - https://arxiv.org/abs/1512.03385\n",
    "* Identity Mappings in Deep Residual Networks by He et al.\n",
    "    - https://arxiv.org/abs/1603.05027\n",
    "* Youtube videos explaining Residual Networks by Andrew Ng\n",
    "    - ResNets https://www.youtube.com/watch?time_continue=1&v=K0uoBKBQ1gA\n",
    "    - Why ResNets work? https://www.youtube.com/watch?v=GSsKdtoatm8\n",
    "    - Network in Network architecture https://www.youtube.com/watch?v=9EZVpLTPGz8\n",
    "\n",
    "###### Below is an image of a residual module (Left) next to an updated residual module (Right) that uses pre-activation.\n",
    "\n",
    "Demostrated in 2016 in a follow up paper (see above), identity mappings helps the ResNets achieve higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/imagenet_resnet_residual_identity.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modules to display images\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML, display\n",
    "# Display two images\n",
    "# display(HTML(\"<table><tr><td><img src='images/imagenet_resnet_residual.png'></td><td><img src='images/imagenet_resnet_residual_identity.png'></td></tr></table>\"))\n",
    "Image(url= \"images/imagenet_resnet_residual_identity.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ResNet50 Architecture Graph\n",
    "\n",
    "Click the link below for a detailed graph of the ResNet50 architecture\n",
    "\n",
    "http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Flowchart\n",
    "\n",
    "In Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data directory structure"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To complete the model testing, we will be using the Dogs vs Cats dataset from Kaggle because CIFAR-10 and MNIST have relatively low pixelation images.\n",
    "\n",
    "Dogs vs Cats data from Kaggle: https://www.kaggle.com/c/dogs-vs-cats/data\n",
    "\n",
    "Example: Dogs vs Cats (Directory Structure)\n",
    "\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    test/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ImageNet\n",
    "\n",
    "What is ImageNet?\n",
    "\n",
    "ImageNet is formally a project aimed at (manually) labeling and categorizing images into almost 22,000 separate object categories for the purpose of computer vision research.\n",
    "\n",
    "When it comes to image classification, the ImageNet challenge is the de facto benchmark for computer vision classification algorithms â€” and the leaderboard for this challenge has been dominated by Convolutional Neural Networks and deep learning techniques since 2012. The state-of-the-art pre-trained networks included in the Keras core library represent some of the highest performing Convolutional Neural Networks on the ImageNet challenge over the past few years. These networks also demonstrate a strong ability to generalize to images outside the ImageNet dataset via transfer learning, such as feature extraction and fine-tuning.\n",
    "\n",
    "The goal of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is to train a model that can correctly classify an input image into 1,000 separate object categories. Models are trained on ~1.2 million training images with another 50,000 images for validation and 100,000 images for testing.\n",
    "\n",
    "These 1,000 image categories represent object classes that we encounter in our day-to-day lives, such as species of dogs, cats, various household objects, types of vehicles, and much more. You can find the full list of object categories in the ILSVRC challenge here.\n",
    "http://image-net.org/challenges/LSVRC/2014/browse-synsets\n",
    "\n",
    "###### This dataset is what the ResNet50 model is trained on. It is good to know the classification labels from that dataset in order for us to work with the pre-trained transfer values. Our primary goal is to take note of the most prominent labels that come with our data type and get some insights into what the model is noticing.\n",
    "\n",
    "###### ImageNet classified synsets useful for ATMA\n",
    "\n",
    "* Letter opener, paper knife, paperknife - 1170 images\n",
    "* Assault rifle, assault gun - 1172 images\n",
    "* Revolver, six-gun, six-shooter - 1223 images\n",
    "* Sweatshirt - 1174 images\n",
    "* Jersey, T-shirt, tee shirt - 1331 images\n",
    "* revolver, six-gun, six-shooter\n",
    "* hatchet\n",
    "* cleaver, meat cleaver, chopper\n",
    "* guillotine\n",
    "* rifle\n",
    "* lighter, light, igniter, ignitor\n",
    "* holster\n",
    "* matchstick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utilities\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# keras.layers\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Activation,\n",
    "    Dense,\n",
    "    Flatten\n",
    ")\n",
    "from keras.layers.convolutional import (\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    AveragePooling2D\n",
    ")\n",
    "from keras.layers.merge import add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#others\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import *\n",
    "from keras.applications import *\n",
    "from keras.models import (\n",
    "    Model,\n",
    "    load_model\n",
    ")\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hyperparameters - for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 2 # Crime, No Crime\n",
    "last_block_layer_of_base_model = 126\n",
    "img_width, img_height = 224, 224 # default parameters for ResNet50 is 224x224\n",
    "num_channels = 3 # 3 color channels for the frames (RBG)\n",
    "batch_size = 32 # we can try 4,8,32,64,128,256,..\n",
    "num_epochs = 50 # number of iterations the algorithm gets trained\n",
    "nadam_lr = 1e-5 # for nadam optimizer\n",
    "learning_rate = 0.045 # for sgd optimizer\n",
    "learning_rate_decay = 0.94 # every two seconds\n",
    "momentum = 0.9 # momentum used for the sgd optimizer\n",
    "transformation_ratio = .05 # how aggressive will the data augmentation/transformation be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original ResNet50 Model - we will use it for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_model = keras.applications.resnet50.ResNet50(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Predicts an image using the original model. Prints out the predicted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(img_path):\n",
    "    img = image.load_img(img_path, target_size=(img_widh, img_height))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    img = image.img_to_array(img) # converts the image to a numpy array\n",
    "    img = np.expand_dims(img, axis=0) # adds a dimension to the image (s1,s2,channels) -> (samples,s1,s2,ch)\n",
    "                                      # this is bcus Keras works with batches of images. The first added dimension is used for that.\n",
    "    img = preprocess_input(img) # sets image to the format the model requires\n",
    "    \n",
    "    predictions = original_model.predict(img)\n",
    "    decoded_labels = decode_predictions(predictions)[0]\n",
    "    # decode_predictions returns a tuple (class, description, probability)\n",
    "    # of the top predictions specified\n",
    "    for image_id, class_name, score in decoded_labels:\n",
    "        print(\"{2:>6.2%} : {1}({0})\".format(image_id class_name, score) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model with many different crime images. Any patterns that ResNet50 specifically recognizes? Anything that it easily identifies? Anything that it misses? How many predicted labels should we consider? Why?\n",
    "\n",
    "# Stopping point for the task given to students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Helper Functions\n",
    "\n",
    "Functions from Hvass-labs Tutorial #10: Fine-tuning https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/10_Fine-Tuning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function used to plot at most 9 images in a 3x3 grid\n",
    "# with the corresponding true and predicted classes below\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred=None, smooth=True):\n",
    "\n",
    "    assert len(images) == len(cls_true)\n",
    "\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    # axes becomes a 3x3 matrix with one axes in each element\n",
    "\n",
    "    # Adjust vertical spacing.\n",
    "    if cls_pred is None:\n",
    "        hspace = 0.3\n",
    "    else:\n",
    "        hspace = 0.6 # extra spacing for the class predicted values\n",
    "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
    "\n",
    "    # Interpolation type.\n",
    "    if smooth:\n",
    "        interpolation = 'spline16'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "\n",
    "    for i, ax in enumerate(axes.flat): # flattens the 3x3 matrix into a 9x1 vector\n",
    "        # There may be less than 9 images, ensure it doesn't crash.\n",
    "        if i < len(images):\n",
    "            # Plot image.\n",
    "            ax.imshow(images[i],\n",
    "                      interpolation=interpolation)\n",
    "\n",
    "            # Name of the true class.\n",
    "            cls_true_name = class_names[cls_true[i]]\n",
    "\n",
    "            # Show true and predicted classes. If predicted value doesnt exist, it doesn't add it on the xlabel\n",
    "            if cls_pred is None:\n",
    "                xlabel = \"True: {0}\".format(cls_true_name)\n",
    "            else:\n",
    "                # Name of the predicted class.\n",
    "                cls_pred_name = class_names[cls_pred[i]]\n",
    "\n",
    "                xlabel = \"True: {0}\\nPred: {1}\".format(cls_true_name, cls_pred_name)\n",
    "\n",
    "            # Show the classes as the label on the x-axis.\n",
    "            ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots the example errors (images) that were mis-classified\n",
    "# (uses plot_images)\n",
    "\n",
    "def plot_example_errors(cls_pred):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Boolean array whether the predicted class is incorrect.\n",
    "    incorrect = (cls_pred != cls_test)\n",
    "\n",
    "    # Get the file-paths for images that were incorrectly classified.\n",
    "    image_paths = np.array(image_paths_test)[incorrect]\n",
    "\n",
    "    # Load the first 9 images.\n",
    "    images = load_images(image_paths=image_paths[0:9])\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = cls_test[incorrect]\n",
    "    \n",
    "    # Plot the 9 images we have loaded and their corresponding classes.\n",
    "    # We have only loaded 9 images so there is no need to slice those again.\n",
    "    plot_images(images=images,\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prints the confusion matrix\n",
    "\n",
    "# Import a function from sklearn to calculate the confusion-matrix.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def print_confusion_matrix(cls_pred):\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_test,  # True class for test-set.\n",
    "                          y_pred=cls_pred)  # Predicted class.\n",
    "\n",
    "    print(\"Confusion matrix:\")\n",
    "    \n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "    \n",
    "    # Print the class-names for easy reference.\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(\"({0}) {1}\".format(i, class_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots the example errors and the confusion matrix \n",
    "# (uses plot_example_errors and plot_confusion_matrix)\n",
    "\n",
    "def example_errors():\n",
    "    # The Keras data-generator for the test-set must be reset\n",
    "    # before processing. This is because the generator will loop\n",
    "    # infinitely and keep an internal index into the dataset.\n",
    "    # So it might start in the middle of the test-set if we do\n",
    "    # not reset it first. This makes it impossible to match the\n",
    "    # predicted classes with the input images.\n",
    "    # If we reset the generator, then it always starts at the\n",
    "    # beginning so we know exactly which input-images were used.\n",
    "    generator_test.reset()\n",
    "    \n",
    "    # Predict the classes for all images in the test-set.\n",
    "    y_pred = new_model.predict_generator(generator_test,\n",
    "                                         steps=steps_test)\n",
    "\n",
    "    # Convert the predicted classes from arrays to integers. (picks the highest score (class prediction) for each image)\n",
    "    cls_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "    # Plot examples of mis-classified images.\n",
    "    plot_example_errors(cls_pred)\n",
    "    \n",
    "    # Print the confusion matrix.\n",
    "    print_confusion_matrix(cls_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loads the images (as numpy arrays) from the directory into memory\n",
    "\n",
    "def load_images(image_paths):\n",
    "    # Load the images from disk.\n",
    "    images = [plt.imread(path) for path in image_paths]\n",
    "\n",
    "    # Convert to a numpy array and return it.\n",
    "    return np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plots the history of the recorded accuracy and loss from the training iterations\n",
    "\n",
    "def plot_training_history(history):\n",
    "    # Get the classification accuracy and loss-value\n",
    "    # for the training-set.\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    loss = history.history['loss']\n",
    "\n",
    "    # Get it for the validation-set (we only use the test-set).\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Plot the accuracy and loss-values for the training-set.\n",
    "    plt.plot(acc, linestyle='-', color='b', label='Training Acc.')\n",
    "    plt.plot(loss, 'o', color='b', label='Training Loss')\n",
    "    \n",
    "    # Plot it for the test-set.\n",
    "    plt.plot(val_acc, linestyle='--', color='r', label='Test Acc.')\n",
    "    plt.plot(val_loss, 'o', color='r', label='Test Loss')\n",
    "\n",
    "    # Plot title and legend.\n",
    "    plt.title('Training and Test Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Ensure the plot shows correctly.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Acquiring the data from a zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Unzip data files into directory path given\\nimport zipfile\\n# 'pwd' gets home folder where notebook opened. Very useful to get paths\\nimport zipfile\\nzip_ref = zipfile.ZipFile('/home/ivargaswhs88/sdata.zip','r')\\n# extracts what is in the zip file, which is already a folder called sdata\\n# so there is no need to create a new directory\\nzip_ref.extractall('/home/ivargaswhs88')\\nzip_ref.close()\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pwd\n",
    "# ^ gets us the home directory\n",
    "\n",
    "'''# Unzip data files into directory path given\n",
    "import zipfile\n",
    "# 'pwd' gets home folder where notebook opened. Very useful to get paths\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('/home/ivargaswhs88/sdata.zip','r')\n",
    "# extracts what is in the zip file, which is already a folder called sdata\n",
    "# so there is no need to create a new directory\n",
    "zip_ref.extractall('/home/ivargaswhs88')\n",
    "zip_ref.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Acquiring the data path directories for each set (training, validation, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir_path = os.path.abspath('/home/ivargaswhs88/sdata')\n",
    "train_dir_path = os.path.join(os.path.abspath(data_dir_path), 'train')\n",
    "validation_dir_path = os.path.join(os.path.abspath(data_dir_path), 'validation')\n",
    "test_dir_path = os.path.join(os.path.abspath(data_dir_path), 'test')\n",
    "\n",
    "# validation for real model we can simply have one full training set\n",
    "# and use a random validation block of close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Knifey Dataset\n",
    "just in case we need to use it for secondary testing\n",
    "Importing the knifey dataset used in the Hvass-Labs tutorials (8,9) \n",
    "import knifey\n",
    "knifey.maybe_download_and_extract()\n",
    "knifey.copy_files()\n",
    "train_dir = knifey.train_dir\n",
    "test_dir = knifey.test_dir\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing the Data \n",
    "Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function used to randomize the image parameters\n",
    "train_genFunction = ImageDataGenerator(rescale=1. / 255)\n",
    "# data generator that uses above function and applies it to the training files\n",
    "train_generator = train_genFunction.flow_from_directory(train_dir_path,\n",
    "                                                        target_size=(img_width, img_height),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        color_mode='rgb',\n",
    "                                                        class_mode='categorical',\n",
    "                                                        horizontal_flip=True,\n",
    "                                                        vertical_flip=True,\n",
    "                                                        shuffle=True)\n",
    "# Additional arguments\n",
    "# rotation_range=transformation_ratio,\n",
    "# shear_range=transformation_ratio,\n",
    "# zoom_range=transformation_ratio,\n",
    "# cval=transformation_ratio,\n",
    "\n",
    "train_iterations = train_generator.n / batch_size\n",
    "# one epoch is when an entire dataset is passed through a NN only one\n",
    "# batch size is the number of training examples in a single batch\n",
    "# iterations are the number of batches needed to complete one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_genFunction = ImageDataGenerator(rescale=1. / 255)\n",
    "validation_generator = validation_genFunction.flow_from_directory(validation_dir_path,\n",
    "                                                             target_size=(img_width, img_height),\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             color_mode='rgb',          \n",
    "                                                             class_mode='categorical',\n",
    "                                                             shuffle=False)\n",
    "validation_iterations = validation_generator.n / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_genFunction = ImageDataGenerator(rescale=1. / 255)\n",
    "test_generator = test_genFunction.flow_from_directory(test_dir_path,\n",
    "                                                     target_size=(img_width, img_height),\n",
    "                                                     batch_size=1,\n",
    "                                                     shuffle=False)\n",
    "test_iterations = test_generator.n / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gets the class numbers for all 3 datasets\n",
    "cls_train = train_generator.classes\n",
    "cls_validation = validation_generator.classes\n",
    "cls_test = test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of class names exported from the directory\n",
    "# this is why it is important to name the directories carefully\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = train_generator.num_classes\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Plot a few images to see that the data is exported well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the first images from the train-set.\n",
    "images = load_images(image_paths=image_paths_train[0:9])\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = cls_train[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true, smooth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATMA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Part 1 - Dense Layers weights (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Loading the model (Incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivargaswhs88/anaconda3/lib/python3.5/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet')\n",
    "# ARGS:\n",
    "# include_top = False -> we will not get the last two fully connected layers\n",
    "# weights = 'imagenet' -> we will get the weights of the model after being trained by the given dataset\n",
    "\n",
    "# Show the model's architecture summary. The name and types of its layers\n",
    "# base_model.summary()\n",
    "\n",
    "# the output shape of the Base model.\n",
    "# base_model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_model.summary()\n",
    "# get a summary. Get the name of last layer and put it below\n",
    "last_conv_layer = model.get_layer('nameOfLayer')\n",
    "# last_conv_layer.output to see what it outputs \n",
    "    # has to be a 4D vector (AllInputs, width, height, # of channels)\n",
    "    # (AllInputs, 7, 7, 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze all ResNet50 convolutional model layers so we only fine-tune the weights of the last added layers we will create. Once those are fine-tuned, then we can fine-tune some of the deeper convolutional layers.\n",
    "\n",
    "This prevents a lot of errors that may propagate from the randomized weights of the layers we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freezing the layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model Completion with Keras Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finishing up the architecture\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # add a pooling layer. turning 2048 features into 1024\n",
    "x = Dense(1024, activation='relu')(x) # a fc player with relu non-linear activation\n",
    "predictions = Dense(num_classes, activation='softmax')(x) # a logistic layer with the number of classes and softmax to normalize the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the model start and end points\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model Compilation --- must be done after freezing the layers\n",
    "# this connects the whole model together and ready for use\n",
    "nadam_optimizer = NAdam(lr=nadam_lr)\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['categorical_accuracy']\n",
    "model.compile(optimizer=nadam_optimizer,\n",
    "              loss=loss,\n",
    "             metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    epochs=num_epochs,\n",
    "                    steps_per_epoch=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the model's performance\n",
    "result = model.evaluate.generator(test_generator, steps=test_iterations)\n",
    "print(\"{0:.2%}\".format(result[1]))\n",
    "# What happens if we print out [0]? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Part 2 - Resnet50 Convolutional Layers weights\n",
    "\n",
    "At this point, the weights of the dense layers have been fine tuned, we can now unfreeze some of the top layers and fine tune their weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unfreezing the layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"train_datagen = ImageDataGenerator(rescale=1. / 255,\\n                   rotation_range=transformation_ratio,\\n                   shear_range=transformation_ratio,\\n                   zoom_range=transformation_ratio,\\n                   cval=transformation_ratio,\\n                   horizontal_flip=True,\\n                   vertical_flip=True)\\n\\nvalidation_datagen = ImageDataGenerator(rescale=1. / 255)\\n\\nvalidation_generator = validation_datagen.flow_from_directory(validation_data_dir,\\n                          target_size=(img_width, img_height),\\n                          batch_size=batch_size,\\n                          class_mode='categorical')\\n\\n\\n# Creates new directory if it does not exist, in the joined path of the train_data_dir path\\n\\nos.makedirs(os.path.join(os.path.abspath(train_data_dir), '../preview'), exist_ok=True)\\n\\n\\n# the data generator takes in:\\n    # The directory of the data\\n    # gets a small batch size of files\\n    # resizes them to the target_size\\n# it spits out a batch of images with different parameters\\n\\ntrain_generator = train_datagen.flow_from_directory(train_data_dir,\\n                    target_size=(img_width, img_height),\\n                    batch_size=batch_size,\\n                    class_mode='categorical')\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates new directory if it does not exist, in the joined path of the train_data_dir path\n",
    "\n",
    "# os.makedirs(os.path.join(os.path.abspath(train_data_dir), '../preview'), exist_ok=True)\n",
    "\n",
    "\n",
    "# the data generator takes in:\n",
    "    # The directory of the data\n",
    "    # gets a small batch size of files\n",
    "    # resizes them to the target_size\n",
    "# it spits out a batch of images with different parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 additional resources\n",
    "\n",
    "* Deep Residual Networks https://github.com/KaimingHe/deep-residual-networks\n",
    "* Graph: http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006\n",
    "* Keras ResNet50 Implementation https://github.com/raghakot/keras-resnet\n",
    "* https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\n",
    "* https://www.quora.com/What-is-the-deep-neural-network-known-as-%E2%80%9CResNet-50%E2%80%9D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import dill\\n\\n# Save session\\ndill.dump_session('saved_sessions/testPickle.db')\\n\\n# Load session\\n# dill.load_session('saved_sessions/testPickle.db')\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import dill\n",
    "\n",
    "# Save session\n",
    "dill.dump_session('saved_sessions/testPickle.db')\n",
    "\n",
    "# Load session\n",
    "# dill.load_session('saved_sessions/testPickle.db')'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
